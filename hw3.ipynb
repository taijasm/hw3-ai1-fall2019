{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kaggle Rossman Competition\n",
    "\n",
    "This was a kaggle competition to forecast sales at a pharmacy chain/dept store in Europe. It was run back in 2015.\n",
    "\n",
    "Rossmann operates over 3,000 drug stores in 7 European countries. As a data analyst for this store, you are tasked with forecasting their daily sales for up to six weeks in advance.\n",
    "\n",
    "You will need to look at various factors influencing the forecast predictions - the primary ones being promotions, competition, school and state holidays, seasonality, and locality.\n",
    "\n",
    "\n",
    "While working through this homework, you will:\n",
    "\n",
    "1. see how to \"grid-search\" when the data is too large to use cross-validation. This is in opposition to the other way we usually do grid search using pipelines. But we still want to use sklearn/dask pipelines as much as possible so that ALL transformations can be used on validation and test sets\n",
    "2. understand some aspects of feature engineering that come in with continuous and categorical variables, and see some of the new features in sklearn 0.20\n",
    "3. capture results from validation\n",
    "4. investigate the use of categorical \"embeddings\" to improve performance of a multi-layer percepton\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path('D:/Documents/AI course/LogisticText/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We engage in some cleaning. A lot of cleaning of this dataset has already been done for us. Some features have been created. In particular we moved from dates to week-of-year, day-of week, etc. For example the 49th and 50th weeks of the year may have higher sales!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data/\"train_clean.csv.gz\", compression='gzip').drop(['index', 'PromoInterval'], axis=1)\n",
    "test_df = pd.read_csv(data/\"test_clean.csv.gz\", compression='gzip').drop(['index', 'PromoInterval'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Events'] = train_df['Events'].fillna('None')\n",
    "test_df['Events'] = test_df['Events'].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We log-transform the dependent variable because it is long-tailed. Plot a histogram of Sales to see this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "plt.hist(train_df['Sales']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resp = np.log(train_df['Sales'].copy())\n",
    "train_df = train_df.drop('Sales', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get some idea about our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>...</th>\n",
       "      <th>AfterStateHoliday</th>\n",
       "      <th>BeforeStateHoliday</th>\n",
       "      <th>AfterPromo</th>\n",
       "      <th>BeforePromo</th>\n",
       "      <th>SchoolHoliday_bw</th>\n",
       "      <th>StateHoliday_bw</th>\n",
       "      <th>Promo_bw</th>\n",
       "      <th>SchoolHoliday_fw</th>\n",
       "      <th>StateHoliday_fw</th>\n",
       "      <th>Promo_fw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek        Date  Customers  Open  Promo  StateHoliday  \\\n",
       "0      1          5  2015-07-31        555     1      1         False   \n",
       "1      2          5  2015-07-31        625     1      1         False   \n",
       "2      3          5  2015-07-31        821     1      1         False   \n",
       "3      4          5  2015-07-31       1498     1      1         False   \n",
       "4      5          5  2015-07-31        559     1      1         False   \n",
       "\n",
       "   SchoolHoliday  Year  Month  ...  AfterStateHoliday  BeforeStateHoliday  \\\n",
       "0              1  2015      7  ...                 57                   0   \n",
       "1              1  2015      7  ...                 67                   0   \n",
       "2              1  2015      7  ...                 57                   0   \n",
       "3              1  2015      7  ...                 67                   0   \n",
       "4              1  2015      7  ...                 57                   0   \n",
       "\n",
       "   AfterPromo  BeforePromo  SchoolHoliday_bw  StateHoliday_bw  Promo_bw  \\\n",
       "0           0            0               5.0              0.0       5.0   \n",
       "1           0            0               5.0              0.0       5.0   \n",
       "2           0            0               5.0              0.0       5.0   \n",
       "3           0            0               5.0              0.0       5.0   \n",
       "4           0            0               5.0              0.0       5.0   \n",
       "\n",
       "   SchoolHoliday_fw  StateHoliday_fw  Promo_fw  \n",
       "0               7.0              0.0       5.0  \n",
       "1               1.0              0.0       1.0  \n",
       "2               5.0              0.0       5.0  \n",
       "3               1.0              0.0       1.0  \n",
       "4               1.0              0.0       1.0  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((844338, 90), (41088, 90))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2015-07-31\n",
       "1         2015-07-31\n",
       "2         2015-07-31\n",
       "3         2015-07-31\n",
       "4         2015-07-31\n",
       "             ...    \n",
       "844333    2013-01-01\n",
       "844334    2013-01-01\n",
       "844335    2013-01-01\n",
       "844336    2013-01-01\n",
       "844337    2013-01-01\n",
       "Name: Date, Length: 844338, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Date # latest date first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Store', 'DayOfWeek', 'Date', 'Customers', 'Open', 'Promo',\n",
       "       'StateHoliday', 'SchoolHoliday', 'Year', 'Month', 'Week', 'Day',\n",
       "       'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start',\n",
       "       'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start',\n",
       "       'Elapsed', 'StoreType', 'Assortment', 'CompetitionDistance',\n",
       "       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n",
       "       'Promo2SinceWeek', 'Promo2SinceYear', 'State', 'file', 'week', 'trend',\n",
       "       'file_DE', 'week_DE', 'trend_DE', 'Date_DE', 'State_DE', 'Month_DE',\n",
       "       'Day_DE', 'Dayofweek_DE', 'Dayofyear_DE', 'Is_month_end_DE',\n",
       "       'Is_month_start_DE', 'Is_quarter_end_DE', 'Is_quarter_start_DE',\n",
       "       'Is_year_end_DE', 'Is_year_start_DE', 'Elapsed_DE', 'Max_TemperatureC',\n",
       "       'Mean_TemperatureC', 'Min_TemperatureC', 'Dew_PointC', 'MeanDew_PointC',\n",
       "       'Min_DewpointC', 'Max_Humidity', 'Mean_Humidity', 'Min_Humidity',\n",
       "       'Max_Sea_Level_PressurehPa', 'Mean_Sea_Level_PressurehPa',\n",
       "       'Min_Sea_Level_PressurehPa', 'Max_VisibilityKm', 'Mean_VisibilityKm',\n",
       "       'Min_VisibilitykM', 'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h',\n",
       "       'Max_Gust_SpeedKm_h', 'Precipitationmm', 'CloudCover', 'Events',\n",
       "       'WindDirDegrees', 'StateName', 'CompetitionOpenSince',\n",
       "       'CompetitionDaysOpen', 'CompetitionMonthsOpen', 'Promo2Since',\n",
       "       'Promo2Days', 'Promo2Weeks', 'AfterSchoolHoliday',\n",
       "       'BeforeSchoolHoliday', 'AfterStateHoliday', 'BeforeStateHoliday',\n",
       "       'AfterPromo', 'BeforePromo', 'SchoolHoliday_bw', 'StateHoliday_bw',\n",
       "       'Promo_bw', 'SchoolHoliday_fw', 'StateHoliday_fw', 'Promo_fw'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of variables and cardinality\n",
    "\n",
    "We make a note of which variables are categorical and which are not. This is a choice. If cardinality is not too high, binning or categorizing can be beneficial. Often this will be true for integer valued variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw', 'Promo', 'SchoolHoliday']\n",
    "\n",
    "cont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "   'AfterStateHoliday', 'BeforeStateHoliday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look for missing data and store the column names where this happend in the continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompetitionDistance 2186\n",
      "CloudCover 68056\n"
     ]
    }
   ],
   "source": [
    "nacols=[]\n",
    "for v in cont_vars:\n",
    "    if np.sum(train_df[v].isnull()) > 0:\n",
    "        nacols.append(v)\n",
    "        print(v, np.sum(train_df[v].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at some cardinalities (unique values) of the continuous data: if we have none below 10, we won't engage in binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompetitionDistance 655\n",
      "Max_TemperatureC 50\n",
      "Mean_TemperatureC 45\n",
      "Min_TemperatureC 40\n",
      "Max_Humidity 50\n",
      "Mean_Humidity 71\n",
      "Min_Humidity 93\n",
      "Max_Wind_SpeedKm_h 42\n",
      "Mean_Wind_SpeedKm_h 27\n",
      "CloudCover 10\n",
      "trend 67\n",
      "trend_DE 38\n",
      "AfterStateHoliday 136\n",
      "BeforeStateHoliday 147\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "for k in cont_vars:\n",
    "    print(k, train_df[k].unique().shape[0])\n",
    "    if train_df[k].unique().shape[0] < 10:\n",
    "        print(train_df[k].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a similar looksie on the categorical variables. Some of these have many levels. Is there really that much information in 1115 store labels. Can we get some compression to increase our signal-to-noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store 1115\n",
      "DayOfWeek 7\n",
      "[5 4 3 2 1 7 6]\n",
      "Year 3\n",
      "[2015 2014 2013]\n",
      "Month 12\n",
      "[ 7  6  5  4  3  2  1 12 11 10  9  8]\n",
      "Day 31\n",
      "[31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10  9  8\n",
      "  7  6  5  4  3  2  1]\n",
      "StateHoliday 2\n",
      "[False  True]\n",
      "CompetitionMonthsOpen 25\n",
      "[24  3 19  9  0 16 17  7 15 22 11 13  2 23 12  4 10  1 14 20  8 18  6 21\n",
      "  5]\n",
      "Promo2Weeks 26\n",
      "[ 0 25 17  8 13 24 16  7 12 23 15  6 11 22 14  5 10 21  4  9 20  3 19  2\n",
      " 18  1]\n",
      "StoreType 4\n",
      "['c' 'a' 'd' 'b']\n",
      "Assortment 3\n",
      "['a' 'c' 'b']\n",
      "CompetitionOpenSinceYear 23\n",
      "[2008 2007 2006 2009 2015 2013 2014 2000 2011 1900 2010 2005 1999 2003\n",
      " 2012 2004 2002 1961 1995 2001 1990 1994 1998]\n",
      "Promo2SinceYear 8\n",
      "[1900 2010 2011 2012 2009 2014 2015 2013]\n",
      "State 12\n",
      "['HE' 'TH' 'NW' 'BE' 'SN' 'SH' 'HB,NI' 'BY' 'BW' 'RP' 'ST' 'HH']\n",
      "Week 52\n",
      "Events 22\n",
      "['Fog' 'None' 'Rain' 'Rain-Thunderstorm' 'Fog-Rain'\n",
      " 'Rain-Hail-Thunderstorm' 'Fog-Rain-Thunderstorm' 'Thunderstorm'\n",
      " 'Rain-Hail' 'Fog-Thunderstorm' 'Rain-Snow' 'Fog-Rain-Hail-Thunderstorm'\n",
      " 'Snow' 'Rain-Snow-Hail' 'Rain-Snow-Hail-Thunderstorm'\n",
      " 'Rain-Snow-Thunderstorm' 'Fog-Rain-Snow' 'Fog-Snow' 'Snow-Hail'\n",
      " 'Fog-Rain-Snow-Hail' 'Fog-Rain-Hail' 'Fog-Snow-Hail']\n",
      "Promo_fw 6\n",
      "[5. 1. 2. 3. 4. 0.]\n",
      "Promo_bw 6\n",
      "[5. 4. 3. 2. 1. 0.]\n",
      "StateHoliday_fw 8\n",
      "[0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "StateHoliday_bw 8\n",
      "[0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "SchoolHoliday_fw 8\n",
      "[7. 1. 5. 4. 2. 3. 0. 6.]\n",
      "SchoolHoliday_bw 8\n",
      "[5. 7. 0. 2. 4. 1. 3. 6.]\n",
      "Promo 2\n",
      "[1 0]\n",
      "SchoolHoliday 2\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "for k in cat_vars:\n",
    "    print(k, train_df[k].unique().shape[0])\n",
    "    if train_df[k].unique().shape[0] < 50:\n",
    "        print(train_df[k].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a validation set\n",
    "\n",
    "The construction of a validation or \"development\" set is not always a `test_train_split` deal. Here we create a validation set of \"latest\" data, cireesponding oin date and size to what we have in the test set. Hopefully this will make sure we have similar distributions of features and outcomes on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41395"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = range(cut)\n",
    "train_idx = list(np.setdiff1d(range(train_df.shape[0]), valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdf = train_df.iloc[train_idx]\n",
    "vadf = train_df.iloc[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((802943, 90), (41395, 90))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf.shape, vadf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Pipelines\n",
    "\n",
    "The pipeline class allows sticking multiple processes into a single scikit-learn estimator. It has fit, predict and score method just like any other estimator (ex. LinearRegression).\n",
    "\n",
    "#### Why Pipelines?\n",
    "\n",
    "1. Pipelines enforce implementation and desired order of steps in your project, which in turn helps in reproducibility and creating a convenient work-flow.\n",
    "2. They prevent data leakage in your validation set during cross-validation by ensuring that data preparation like standardization is constrained to each fold of your cross validation procedure.\n",
    "\n",
    "Ok, now we'll use the new `ColumnTransformer`, with imputation, missing-data indicators, the new `OrdinalEncoder`, and the usual Standard Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer,MissingIndicator\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "impu = SimpleImputer(strategy=\"median\") # create a median imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the missing indicator separately as it creates a new column. It is possible to do this in the pipeline flow in `sklearn` using a union, but subsequent scaling wants to scale this indicator since the categorical list does not include the new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = MissingIndicator() # create, fit, and transform a missingness indicator\n",
    "mi.fit(trdf[nacols])\n",
    "Xtrmi = mi.transform(trdf[nacols])\n",
    "Xvami = mi.transform(vadf[nacols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrmi[4460,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "ss = StandardScaler()\n",
    "oe = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdf_cat = trdf[cat_vars]\n",
    "trdf_cont = trdf[cont_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct two pipelines, one for categoricals and one for continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_pipe = Pipeline([(\"imp\",impu), (\"scale\", ss)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([(\"categorify\", oe)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And combine them here in a transformer list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [('cat', cat_pipe, cat_vars),\n",
    "                    ('cont', cont_pipe, cont_vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use a `ColumnTransformer` to combine these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer(transformers=transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('cat',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('categorify',\n",
       "                                                  OrdinalEncoder(categories='auto',\n",
       "                                                                 dtype=<class 'numpy.float64'>))],\n",
       "                                          verbose=False),\n",
       "                                 ['Store', 'DayOfWeek', 'Year', 'Month', 'Day',\n",
       "                                  'StateHoliday', 'CompetitionMonthsOpen',\n",
       "                                  'Promo2Weeks', 'StoreType', 'Assort...\n",
       "                                                                verbose=0)),\n",
       "                                                 ('scale',\n",
       "                                                  StandardScaler(copy=True,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True))],\n",
       "                                          verbose=False),\n",
       "                                 ['CompetitionDistance', 'Max_TemperatureC',\n",
       "                                  'Mean_TemperatureC', 'Min_TemperatureC',\n",
       "                                  'Max_Humidity', 'Mean_Humidity',\n",
       "                                  'Min_Humidity', 'Max_Wind_SpeedKm_h',\n",
       "                                  'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend',\n",
       "                                  'trend_DE', 'AfterStateHoliday',\n",
       "                                  'BeforeStateHoliday'])],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.fit(trdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = ct.transform(trdf)\n",
    "Xval = ct.transform(vadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((802943, 37), (802943, 2))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Xtrmi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate the old indicators back in. The transformer lists all the categoricals first, since thats the first item in `transformers`, so we pre-pend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802943, 39)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = np.concatenate([Xtrmi, Xtr], axis=1)\n",
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41395, 39)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvalid = np.concatenate([Xvami, Xval], axis=1)\n",
    "Xvalid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn-pipelines lose our nice pandas names. so we bring them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, ('CompetitionDistance_missing', 'cont')),\n",
       "  (1, ('CloudCover_missing', 'cont')),\n",
       "  (2, ('Store', 'cat')),\n",
       "  (3, ('DayOfWeek', 'cat')),\n",
       "  (4, ('Year', 'cat')),\n",
       "  (5, ('Month', 'cat')),\n",
       "  (6, ('Day', 'cat')),\n",
       "  (7, ('StateHoliday', 'cat')),\n",
       "  (8, ('CompetitionMonthsOpen', 'cat')),\n",
       "  (9, ('Promo2Weeks', 'cat')),\n",
       "  (10, ('StoreType', 'cat')),\n",
       "  (11, ('Assortment', 'cat')),\n",
       "  (12, ('CompetitionOpenSinceYear', 'cat')),\n",
       "  (13, ('Promo2SinceYear', 'cat')),\n",
       "  (14, ('State', 'cat')),\n",
       "  (15, ('Week', 'cat')),\n",
       "  (16, ('Events', 'cat')),\n",
       "  (17, ('Promo_fw', 'cat')),\n",
       "  (18, ('Promo_bw', 'cat')),\n",
       "  (19, ('StateHoliday_fw', 'cat')),\n",
       "  (20, ('StateHoliday_bw', 'cat')),\n",
       "  (21, ('SchoolHoliday_fw', 'cat')),\n",
       "  (22, ('SchoolHoliday_bw', 'cat')),\n",
       "  (23, ('Promo', 'cat')),\n",
       "  (24, ('SchoolHoliday', 'cat')),\n",
       "  (25, ('CompetitionDistance', 'cont')),\n",
       "  (26, ('Max_TemperatureC', 'cont')),\n",
       "  (27, ('Mean_TemperatureC', 'cont')),\n",
       "  (28, ('Min_TemperatureC', 'cont')),\n",
       "  (29, ('Max_Humidity', 'cont')),\n",
       "  (30, ('Mean_Humidity', 'cont')),\n",
       "  (31, ('Min_Humidity', 'cont')),\n",
       "  (32, ('Max_Wind_SpeedKm_h', 'cont')),\n",
       "  (33, ('Mean_Wind_SpeedKm_h', 'cont')),\n",
       "  (34, ('CloudCover', 'cont')),\n",
       "  (35, ('trend', 'cont')),\n",
       "  (36, ('trend_DE', 'cont')),\n",
       "  (37, ('AfterStateHoliday', 'cont')),\n",
       "  (38, ('BeforeStateHoliday', 'cont'))],\n",
       " 39)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = trdf.columns\n",
    "actcols = []\n",
    "actcolcount = 0\n",
    "nacols_cat = []\n",
    "for k in nacols:\n",
    "    actcols.append((k+'_missing', 'cont'))\n",
    "    nacols_cat.append(k+'_missing')\n",
    "    actcolcount+=1\n",
    "for k in cat_vars+cont_vars:\n",
    "    if k in cat_vars:\n",
    "        actcols.append((k, \"cat\"))\n",
    "        actcolcount+=1\n",
    "    if k in cont_vars:\n",
    "        actcols.append((k, \"cont\"))\n",
    "        actcolcount+=1\n",
    "        \n",
    "list(enumerate(actcols)), actcolcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to learn\n",
    "We will follow two approches here:\n",
    "1. Gradient Boosting Regression Trees\n",
    "2. Multi-layer Perceptron Model using Entity Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression Trees(GBRT)\n",
    "We first split the y (the log of the y, really)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((802943,), (41395,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain = train_resp[train_idx]\n",
    "yvalid = train_resp[list(valid_idx)]\n",
    "ytrain.shape, yvalid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and import what we need to for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peter Prettenhofer, who wrote sklearns GBRT implementation writes in his pydata14 talk (worth watching!) ([link](https://www.youtube.com/watch?v=-5l3g91NZfQ) here)\n",
    "\n",
    ">Hyperparameter tuning - I usually follow this recipe to tune the hyperparameters:\n",
    "\n",
    "> \n",
    "- Pick n_estimators as large as (computationally) possible (e.g. 3000)\n",
    "- Tune max_depth, learning_rate, min_samples_leaf, and max_features via grid search\n",
    "- A lower learning_rate requires a higher number of n_estimators. Thus increase n_estimators even more and tune learning_rate again holding the other parameters fixed\n",
    "\n",
    ">This last point is a trade-off between number of iterations or runtime against accuracy. And keep in mind that it might lead to overfitting.\n",
    "\n",
    "Let me add however, that poor learners do rather well. So you might want to not cross-validate max_depth. And min_samples_per_leaf is not independent either, so if you do use cross-val, you might just use one of those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `ParameterGrid` here to construct the entire grid for us! We put the output in a list of dictionaries and then save it in a dataframe. We might want to persist such dataframes to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [0.1, 0.01],\n",
    "              'max_depth': [1,2, 3],\n",
    "              'max_features': [0.2, 0.6]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 1, 'max_features': 0.2}\n",
      "MSE 0.11941509177969586 0.12841535991083233\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'max_features': 0.6}\n",
      "MSE 0.11963711619941164 0.12808001640622907\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'max_features': 0.2}\n",
      "MSE 0.10845097990678487 0.11367749112232332\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'max_features': 0.6}\n",
      "MSE 0.10778344629092591 0.11094270785281257\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 0.2}\n",
      "MSE 0.09806334855272439 0.099923695399812\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 0.6}\n",
      "MSE 0.0892170696499436 0.0892691903913021\n",
      "{'learning_rate': 0.01, 'max_depth': 1, 'max_features': 0.2}\n",
      "MSE 0.1494305248759741 0.15908599850467375\n",
      "{'learning_rate': 0.01, 'max_depth': 1, 'max_features': 0.6}\n",
      "MSE 0.13858267081842604 0.15046440065581673\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'max_features': 0.2}\n",
      "MSE 0.13850808249880406 0.1471613522326658\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'max_features': 0.6}\n",
      "MSE 0.130423477550135 0.1406105959851407\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'max_features': 0.2}\n",
      "MSE 0.12920959450020422 0.13736402274153142\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'max_features': 0.6}\n",
      "MSE 0.12272781477369149 0.1320933521561417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'learning_rate': 0.1,\n",
       "  'max_depth': 1,\n",
       "  'max_features': 0.2,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.11941509177969586,\n",
       "  'msetr': 0.12841535991083233},\n",
       " {'learning_rate': 0.1,\n",
       "  'max_depth': 1,\n",
       "  'max_features': 0.6,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.11963711619941164,\n",
       "  'msetr': 0.12808001640622907},\n",
       " {'learning_rate': 0.1,\n",
       "  'max_depth': 2,\n",
       "  'max_features': 0.2,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.10845097990678487,\n",
       "  'msetr': 0.11367749112232332},\n",
       " {'learning_rate': 0.1,\n",
       "  'max_depth': 2,\n",
       "  'max_features': 0.6,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.10778344629092591,\n",
       "  'msetr': 0.11094270785281257},\n",
       " {'learning_rate': 0.1,\n",
       "  'max_depth': 3,\n",
       "  'max_features': 0.2,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.09806334855272439,\n",
       "  'msetr': 0.099923695399812},\n",
       " {'learning_rate': 0.1,\n",
       "  'max_depth': 3,\n",
       "  'max_features': 0.6,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.0892170696499436,\n",
       "  'msetr': 0.0892691903913021},\n",
       " {'learning_rate': 0.01,\n",
       "  'max_depth': 1,\n",
       "  'max_features': 0.2,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.1494305248759741,\n",
       "  'msetr': 0.15908599850467375},\n",
       " {'learning_rate': 0.01,\n",
       "  'max_depth': 1,\n",
       "  'max_features': 0.6,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.13858267081842604,\n",
       "  'msetr': 0.15046440065581673},\n",
       " {'learning_rate': 0.01,\n",
       "  'max_depth': 2,\n",
       "  'max_features': 0.2,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.13850808249880406,\n",
       "  'msetr': 0.1471613522326658},\n",
       " {'learning_rate': 0.01,\n",
       "  'max_depth': 2,\n",
       "  'max_features': 0.6,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.130423477550135,\n",
       "  'msetr': 0.1406105959851407},\n",
       " {'learning_rate': 0.01,\n",
       "  'max_depth': 3,\n",
       "  'max_features': 0.2,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.12920959450020422,\n",
       "  'msetr': 0.13736402274153142},\n",
       " {'learning_rate': 0.01,\n",
       "  'max_depth': 3,\n",
       "  'max_features': 0.6,\n",
       "  'n_estimators': 200,\n",
       "  'mse': 0.12272781477369149,\n",
       "  'msetr': 0.1320933521561417}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "ds=[]\n",
    "for p in ParameterGrid(param_grid):\n",
    "    print(p)\n",
    "    gb = GradientBoostingRegressor(n_estimators=200)\n",
    "    gb.set_params(**p)\n",
    "    gb.fit(Xtrain, ytrain)\n",
    "    ypred = gb.predict(Xvalid)\n",
    "    ypredtrain = gb.predict(Xtrain)\n",
    "    d = p.copy()\n",
    "    d['n_estimators']=200\n",
    "    d['mse'] = mean_squared_error(ypred, yvalid)\n",
    "    d['msetr'] = mean_squared_error(ypredtrain, ytrain)\n",
    "    print(\"MSE\", d['mse'], d['msetr'])\n",
    "    ds.append(d)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>mse</th>\n",
       "      <th>msetr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.089217</td>\n",
       "      <td>0.089269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.098063</td>\n",
       "      <td>0.099924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.107783</td>\n",
       "      <td>0.110943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.108451</td>\n",
       "      <td>0.113677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.119415</td>\n",
       "      <td>0.128415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.119637</td>\n",
       "      <td>0.128080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.122728</td>\n",
       "      <td>0.132093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.129210</td>\n",
       "      <td>0.137364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.130423</td>\n",
       "      <td>0.140611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.138508</td>\n",
       "      <td>0.147161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>200</td>\n",
       "      <td>0.138583</td>\n",
       "      <td>0.150464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.149431</td>\n",
       "      <td>0.159086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  max_depth  max_features  n_estimators       mse     msetr\n",
       "5            0.10          3           0.6           200  0.089217  0.089269\n",
       "4            0.10          3           0.2           200  0.098063  0.099924\n",
       "3            0.10          2           0.6           200  0.107783  0.110943\n",
       "2            0.10          2           0.2           200  0.108451  0.113677\n",
       "0            0.10          1           0.2           200  0.119415  0.128415\n",
       "1            0.10          1           0.6           200  0.119637  0.128080\n",
       "11           0.01          3           0.6           200  0.122728  0.132093\n",
       "10           0.01          3           0.2           200  0.129210  0.137364\n",
       "9            0.01          2           0.6           200  0.130423  0.140611\n",
       "8            0.01          2           0.2           200  0.138508  0.147161\n",
       "7            0.01          1           0.6           200  0.138583  0.150464\n",
       "6            0.01          1           0.2           200  0.149431  0.159086"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsdf = pd.DataFrame.from_records(ds)\n",
    "dsdf.sort_values('mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Multi-Layer Perceptron Model\n",
    "\n",
    "This is based on the 3rd prize winning entry, whose authors wrote a [paper](https://arxiv.org/pdf/1604.06737.pdf) afterwords.\n",
    "\n",
    "What we are first going to do is to reduce the cardinality dimensionality of our categoricals by using **embeddings**. \n",
    "\n",
    "#### What are embeddings?\n",
    "An embedding is a mapping of discrete - categorical - variable to a vector of continuous variables. Neural Network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space\n",
    "This technique is often used not only in recommender systems(via matrix factorization), but also in NLP models such as `word2vec`.\n",
    "\n",
    "In our problem, consider `store_id` as an example. This is a categorical predictor and we usually **one-hot encode** this - a single store is a length 3000 but-vector with one bit flipped on.\n",
    "\n",
    "#### Problems with One-hot encoding\n",
    "1. The 3000 stores will have some commonalities, but the one-hot encoding doesn't represent this. The dot-product(cosine similarity) of any 2 one-hot encoded stores will be 0\n",
    "2. The store_id variable has high cardinality(3000 unique categories) - in this case, the dimensionality of the transformed variable becomes unmanageable\n",
    "\n",
    "Using embeddings for `store_id` will enable us to learn the store 'personalities', which can then be used later in other models for sales predictions, or even for other tasks.\n",
    "\n",
    "#### Training an embedding\n",
    "\n",
    "- Normally you would do a linear or MLP regression with sales as the target, and both continuous and categorical features\n",
    "- We need to replace the 1-hot encoded categorical features by \"lower-width\" embedding features\n",
    "- This is equivalent to considering a neural network with the output of an additional **Embedding Layer** concatenated in\n",
    "- The Embedding Layer is simply a Linear Regression\n",
    "\n",
    "![](./images/embmlp.png)\n",
    "\n",
    "Here we divide the cardinality by 2 and add 1 to get the embedding (this is a heuristic). If the cardinality is high, we clamp the size of the latent space down at 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CompetitionDistance_missing': (2, 2),\n",
       " 'CloudCover_missing': (2, 2),\n",
       " 'Store': (1115, 50),\n",
       " 'DayOfWeek': (7, 4),\n",
       " 'Year': (3, 2),\n",
       " 'Month': (12, 7),\n",
       " 'Day': (31, 16),\n",
       " 'StateHoliday': (2, 2),\n",
       " 'CompetitionMonthsOpen': (25, 13),\n",
       " 'Promo2Weeks': (26, 14),\n",
       " 'StoreType': (4, 3),\n",
       " 'Assortment': (3, 2),\n",
       " 'CompetitionOpenSinceYear': (23, 12),\n",
       " 'Promo2SinceYear': (8, 5),\n",
       " 'State': (12, 7),\n",
       " 'Week': (52, 27),\n",
       " 'Events': (22, 12),\n",
       " 'Promo_fw': (6, 4),\n",
       " 'Promo_bw': (6, 4),\n",
       " 'StateHoliday_fw': (8, 5),\n",
       " 'StateHoliday_bw': (8, 5),\n",
       " 'SchoolHoliday_fw': (8, 5),\n",
       " 'SchoolHoliday_bw': (8, 5),\n",
       " 'Promo': (2, 2),\n",
       " 'SchoolHoliday': (2, 2)}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards={}\n",
    "for k in nacols_cat:\n",
    "    cards[k] = (2,2)\n",
    "for k in cat_vars :\n",
    "    embed_sz_base = trdf[k].unique().size//2 + 1\n",
    "    embed_sz = (embed_sz_base <=50)*embed_sz_base + 50*((embed_sz_base > 50))\n",
    "    cards[k] = (trdf[k].unique().size, embed_sz)\n",
    "cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to be careful (very book-keepy) in constructing a model in Keras. We use the Keras Functional API as opposed to the Sequential API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Activation, Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "def build_keras_model():\n",
    "    input_cat = []\n",
    "    output_embeddings = []\n",
    "    for k in nacols_cat+cat_vars:\n",
    "        print('{}_embedding'.format(k))\n",
    "        input_1d = Input(shape=(1,))\n",
    "        output_1d = Embedding(cards[k][0], cards[k][1], name='{}_embedding'.format(k))(input_1d)\n",
    "        output = Reshape(target_shape=(cards[k][1],))(output_1d)\n",
    "        input_cat.append(input_1d)\n",
    "        output_embeddings.append(output)\n",
    "\n",
    "    main_input = Input(shape=(len(cont_vars),), name='main_input')\n",
    "    output_model = Concatenate()([main_input, *output_embeddings])\n",
    "    output_model = Dense(1000, kernel_initializer=\"uniform\")(output_model)\n",
    "    output_model = Activation('relu')(output_model)\n",
    "    output_model = Dense(500, kernel_initializer=\"uniform\")(output_model)\n",
    "    output_model = Activation('relu')(output_model)\n",
    "    output_model = Dense(1)(output_model)\n",
    "    kmodel = KerasModel(\n",
    "        inputs=[*input_cat, main_input], \n",
    "        outputs=output_model\n",
    ")\n",
    "    kmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return kmodel\n",
    "\n",
    "def fitmodel(kmodel, Xtr, ytr, Xval, yval, epochs, bs):\n",
    "    h = kmodel.fit(Xtr, ytr, validation_data=(Xval, yval),\n",
    "                       epochs=epochs, batch_size=bs)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data input needs to match our construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "list_cat_trains=[]\n",
    "list_cat_valids=[]\n",
    "catlen=len(nacols_cat+cat_vars)\n",
    "for i in range(catlen):\n",
    "    list_cat_trains.append(Xtrain[:,i])\n",
    "    list_cat_valids.append(Xvalid[:,i])\n",
    "cont_train=Xtrain[:,catlen:]\n",
    "cont_valid=Xvalid[:,catlen:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802943, 14)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run (only a little bit for now!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompetitionDistance_missing_embedding\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "CloudCover_missing_embedding\n",
      "Store_embedding\n",
      "DayOfWeek_embedding\n",
      "Year_embedding\n",
      "Month_embedding\n",
      "Day_embedding\n",
      "StateHoliday_embedding\n",
      "CompetitionMonthsOpen_embedding\n",
      "Promo2Weeks_embedding\n",
      "StoreType_embedding\n",
      "Assortment_embedding\n",
      "CompetitionOpenSinceYear_embedding\n",
      "Promo2SinceYear_embedding\n",
      "State_embedding\n",
      "Week_embedding\n",
      "Events_embedding\n",
      "Promo_fw_embedding\n",
      "Promo_bw_embedding\n",
      "StateHoliday_fw_embedding\n",
      "StateHoliday_bw_embedding\n",
      "SchoolHoliday_fw_embedding\n",
      "SchoolHoliday_bw_embedding\n",
      "Promo_embedding\n",
      "SchoolHoliday_embedding\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 802943 samples, validate on 41395 samples\n",
      "Epoch 1/10\n",
      "802943/802943 [==============================] - 148s 184us/step - loss: 0.2113 - val_loss: 0.0213\n",
      "Epoch 2/10\n",
      "802943/802943 [==============================] - 136s 169us/step - loss: 0.0197 - val_loss: 0.0192\n",
      "Epoch 3/10\n",
      "802943/802943 [==============================] - 137s 171us/step - loss: 0.0156 - val_loss: 0.0165\n",
      "Epoch 4/10\n",
      "802943/802943 [==============================] - 138s 172us/step - loss: 0.0134 - val_loss: 0.0157\n",
      "Epoch 5/10\n",
      "802943/802943 [==============================] - 131s 164us/step - loss: 0.0121 - val_loss: 0.0148\n",
      "Epoch 6/10\n",
      "802943/802943 [==============================] - 134s 166us/step - loss: 0.0108 - val_loss: 0.0151\n",
      "Epoch 7/10\n",
      "802943/802943 [==============================] - 132s 164us/step - loss: 0.0101 - val_loss: 0.0156\n",
      "Epoch 8/10\n",
      "802943/802943 [==============================] - 132s 165us/step - loss: 0.0094 - val_loss: 0.0166\n",
      "Epoch 9/10\n",
      "802943/802943 [==============================] - 130s 162us/step - loss: 0.0090 - val_loss: 0.0137\n",
      "Epoch 10/10\n",
      "802943/802943 [==============================] - 131s 163us/step - loss: 0.0086 - val_loss: 0.0132\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "emodel = build_keras_model()\n",
    "history = fitmodel(emodel, [*list_cat_trains, cont_train], ytrain, [*list_cat_valids, cont_valid], yvalid, 10, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Lets do the GBM on dask. And later, at your leisure, you'll want to `ParameterGrid` Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
